---
layout: post
title:  "Attention Is All You Need 공부"
date:   2021-02-25 15:00:00 +0530
tag:
- transformer
- Attention Is All You Need
---
sample script ~~asdasd~~ ~asd~~asd~~dd//asd//  /*asd*/ asd *asd **asd

```javascript
const Razorpay = require('razorpay');

let rzp = Razorpay({
	key_id: 'KEY_ID',
	secret: 'name'
});

// capture request
rzp.capture(payment_id, cost)
	.then(function (data) {
		return 2;
	})
```
# Ing..


# Intro

Recurrent neural networks는 t번째의 output을 얻기위해 t번째 input과 t-1번쨰 hidden state가 필요하다. 이는 모델의 병렬화를 막고 Long term dependency문제가 있다.

이를 방지하기 위해 RNN 대신 Attention을 사용하였다.


sample link [blog]

[blog]: https://supernova817.github.io
